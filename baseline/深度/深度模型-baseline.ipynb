{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据地址"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = 0\n",
    "nrows = None\n",
    "train_path = '../../data/underexpose_train'  \n",
    "test_path = '../../data/underexpose_test' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_train = pd.read_csv(\n",
    "                        train_path + '/underexpose_train_click-{phase}.csv'.format(phase=phase)\n",
    "                        ,header=None\n",
    "                        ,nrows=nrows\n",
    "                        ,names=['user_id', 'item_id', 'time']\n",
    "                        ,sep=','\n",
    "                        ,dtype={'user_id':np.str,'item_id':np.str,'time':np.float}\n",
    "                        ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_test = pd.read_csv(\n",
    "                        test_path + '/underexpose_test_click-{phase}/underexpose_test_click-{phase}.csv'.format(phase=phase)\n",
    "                        ,header=None\n",
    "                        ,nrows=nrows\n",
    "                        ,names=['user_id', 'item_id', 'time']\n",
    "                        ,sep=','\n",
    "                        ,dtype={'user_id':np.str,'item_id':np.str,'time':np.float}\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 合并数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_all = click_train.append(click_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 处理数据 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_item_id = set(click_all['item_id'])\n",
    "dict_item_id_map = dict(zip(set_item_id,range(1,len(set_item_id)+1)))\n",
    "click_all['item_id_map'] = click_all['item_id'].map(dict_item_id_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 按时间排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_all = click_all.sort_values('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>time</th>\n",
       "      <th>item_id_map</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>89202</th>\n",
       "      <td>20937</td>\n",
       "      <td>18599</td>\n",
       "      <td>0.98374</td>\n",
       "      <td>31836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>25129</td>\n",
       "      <td>3852</td>\n",
       "      <td>0.98374</td>\n",
       "      <td>24011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97167</th>\n",
       "      <td>12136</td>\n",
       "      <td>28195</td>\n",
       "      <td>0.98374</td>\n",
       "      <td>11962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9827</th>\n",
       "      <td>22528</td>\n",
       "      <td>48057</td>\n",
       "      <td>0.98374</td>\n",
       "      <td>16243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188473</th>\n",
       "      <td>23813</td>\n",
       "      <td>85991</td>\n",
       "      <td>0.98374</td>\n",
       "      <td>24975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id item_id     time  item_id_map\n",
       "89202    20937   18599  0.98374        31836\n",
       "247      25129    3852  0.98374        24011\n",
       "97167    12136   28195  0.98374        11962\n",
       "9827     22528   48057  0.98374        16243\n",
       "188473   23813   85991  0.98374        24975"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "click_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40776, 1, 40776)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_item_id, min_item_id = click_all['item_id_map'].max(), click_all['item_id_map'].min()\n",
    "max_item_id, min_item_id, len(set_item_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 时间归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_all['time'] = (click_all['time'] - np.min(click_all['time'])) / (np.max(click_all['time']) - np.min(click_all['time']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['user_id', 'item_id', 'time', 'item_id_map'], dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "click_all.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 聚合成一列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_all_gb = click_all.groupby(['user_id']).agg({'item_id_map':lambda x:list(x), 'time':lambda x:list(x)}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id_map</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[38553, 4746, 4977, 1673, 30860, 14976, 29329,...</td>\n",
       "      <td>[0.008894984779548962, 0.07786149982061645, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>[12476, 37327, 27652, 571, 28754, 15324, 10207...</td>\n",
       "      <td>[0.08402780189097342, 0.08881095408378337, 0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>[31950, 17557, 14981, 23451, 5671, 8651, 24658...</td>\n",
       "      <td>[0.10338321932372357, 0.15522529717377634, 0.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000</td>\n",
       "      <td>[2698, 26905, 30914]</td>\n",
       "      <td>[0.08657418660304175, 0.20507945877707037, 0.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10002</td>\n",
       "      <td>[10232, 2684, 34467, 27480, 2701, 20326, 35051...</td>\n",
       "      <td>[0.46173767607676885, 0.46441138002507715, 0.4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id                                        item_id_map  \\\n",
       "0       1  [38553, 4746, 4977, 1673, 30860, 14976, 29329,...   \n",
       "1      10  [12476, 37327, 27652, 571, 28754, 15324, 10207...   \n",
       "2     100  [31950, 17557, 14981, 23451, 5671, 8651, 24658...   \n",
       "3   10000                               [2698, 26905, 30914]   \n",
       "4   10002  [10232, 2684, 34467, 27480, 2701, 20326, 35051...   \n",
       "\n",
       "                                                time  \n",
       "0  [0.008894984779548962, 0.07786149982061645, 0....  \n",
       "1  [0.08402780189097342, 0.08881095408378337, 0.2...  \n",
       "2  [0.10338321932372357, 0.15522529717377634, 0.3...  \n",
       "3  [0.08657418660304175, 0.20507945877707037, 0.5...  \n",
       "4  [0.46173767607676885, 0.46441138002507715, 0.4...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "click_all_gb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>time</th>\n",
       "      <th>item_id_map</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19829</th>\n",
       "      <td>1</td>\n",
       "      <td>78142</td>\n",
       "      <td>0.008895</td>\n",
       "      <td>38553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236132</th>\n",
       "      <td>1</td>\n",
       "      <td>26646</td>\n",
       "      <td>0.077861</td>\n",
       "      <td>4746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20480</th>\n",
       "      <td>1</td>\n",
       "      <td>89568</td>\n",
       "      <td>0.108965</td>\n",
       "      <td>4977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19709</th>\n",
       "      <td>1</td>\n",
       "      <td>76240</td>\n",
       "      <td>0.141049</td>\n",
       "      <td>1673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108033</th>\n",
       "      <td>1</td>\n",
       "      <td>87533</td>\n",
       "      <td>0.228535</td>\n",
       "      <td>30860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56362</th>\n",
       "      <td>1</td>\n",
       "      <td>78380</td>\n",
       "      <td>0.228631</td>\n",
       "      <td>14976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159250</th>\n",
       "      <td>1</td>\n",
       "      <td>85492</td>\n",
       "      <td>0.622863</td>\n",
       "      <td>29329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20968</th>\n",
       "      <td>1</td>\n",
       "      <td>97795</td>\n",
       "      <td>0.630401</td>\n",
       "      <td>12251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111177</th>\n",
       "      <td>1</td>\n",
       "      <td>18522</td>\n",
       "      <td>0.673606</td>\n",
       "      <td>880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3123</th>\n",
       "      <td>1</td>\n",
       "      <td>47611</td>\n",
       "      <td>0.674045</td>\n",
       "      <td>20815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214540</th>\n",
       "      <td>1</td>\n",
       "      <td>31443</td>\n",
       "      <td>0.675455</td>\n",
       "      <td>16026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74513</th>\n",
       "      <td>1</td>\n",
       "      <td>17887</td>\n",
       "      <td>0.706251</td>\n",
       "      <td>38699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84964</th>\n",
       "      <td>1</td>\n",
       "      <td>69359</td>\n",
       "      <td>0.927642</td>\n",
       "      <td>9499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id item_id      time  item_id_map\n",
       "19829        1   78142  0.008895        38553\n",
       "236132       1   26646  0.077861         4746\n",
       "20480        1   89568  0.108965         4977\n",
       "19709        1   76240  0.141049         1673\n",
       "108033       1   87533  0.228535        30860\n",
       "56362        1   78380  0.228631        14976\n",
       "159250       1   85492  0.622863        29329\n",
       "20968        1   97795  0.630401        12251\n",
       "111177       1   18522  0.673606          880\n",
       "3123         1   47611  0.674045        20815\n",
       "214540       1   31443  0.675455        16026\n",
       "74513        1   17887  0.706251        38699\n",
       "84964        1   69359  0.927642         9499"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "click_all[click_all['user_id']=='1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练和预测数据打标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_test = set(click_test['user_id'])\n",
    "user_train = set(click_all['user_id']) - user_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_all_gb['flag'] = click_all_gb['user_id'].map(lambda x: 'test' if x in user_test else 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id_map</th>\n",
       "      <th>time</th>\n",
       "      <th>flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[38553, 4746, 4977, 1673, 30860, 14976, 29329,...</td>\n",
       "      <td>[0.008894984779548962, 0.07786149982061645, 0....</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>[12476, 37327, 27652, 571, 28754, 15324, 10207...</td>\n",
       "      <td>[0.08402780189097342, 0.08881095408378337, 0.2...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>[31950, 17557, 14981, 23451, 5671, 8651, 24658...</td>\n",
       "      <td>[0.10338321932372357, 0.15522529717377634, 0.3...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000</td>\n",
       "      <td>[2698, 26905, 30914]</td>\n",
       "      <td>[0.08657418660304175, 0.20507945877707037, 0.5...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10002</td>\n",
       "      <td>[10232, 2684, 34467, 27480, 2701, 20326, 35051...</td>\n",
       "      <td>[0.46173767607676885, 0.46441138002507715, 0.4...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id                                        item_id_map  \\\n",
       "0       1  [38553, 4746, 4977, 1673, 30860, 14976, 29329,...   \n",
       "1      10  [12476, 37327, 27652, 571, 28754, 15324, 10207...   \n",
       "2     100  [31950, 17557, 14981, 23451, 5671, 8651, 24658...   \n",
       "3   10000                               [2698, 26905, 30914]   \n",
       "4   10002  [10232, 2684, 34467, 27480, 2701, 20326, 35051...   \n",
       "\n",
       "                                                time   flag  \n",
       "0  [0.008894984779548962, 0.07786149982061645, 0....  train  \n",
       "1  [0.08402780189097342, 0.08881095408378337, 0.2...  train  \n",
       "2  [0.10338321932372357, 0.15522529717377634, 0.3...  train  \n",
       "3  [0.08657418660304175, 0.20507945877707037, 0.5...  train  \n",
       "4  [0.46173767607676885, 0.46441138002507715, 0.4...  train  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "click_all_gb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 获取训练和预测数据集合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(click_all_gb, list_label_loc=[0,1,2], seq_length=10):\n",
    "    list_seq_feature_item = []\n",
    "    list_seq_feature_time = []\n",
    "    list_label = []\n",
    "    list_user = []\n",
    "    \n",
    "    # 0 1 2\n",
    "    for loc in list_label_loc:\n",
    "        for i, row in tqdm(click_all_gb.iterrows()):\n",
    "            user_id, list_item_id, list_time, flag = row['user_id'], row['item_id_map'][::-1], row['time'][::-1], row['flag']\n",
    "            \n",
    "            if (flag == 'test') and (loc == list_label_loc[0]):\n",
    "                list_label.append(-1)\n",
    "                list_user.append(user_id)\n",
    "                seq_feature_item_test = list_item_id[0:loc+seq_length]\n",
    "                len_seq_feature_item_test = len(seq_feature_item_test)\n",
    "                if len_seq_feature_item_test < seq_length:\n",
    "                    seq_feature_item_test += [0] * (seq_length - len_seq_feature_item_test) \n",
    "                    \n",
    "                seq_feature_time_test = list_time[0:loc+seq_length]\n",
    "                len_seq_feature_time_test = len(seq_feature_time_test)\n",
    "                if len_seq_feature_time_test < seq_length:\n",
    "                    seq_feature_time_test += [0] * (seq_length - len_seq_feature_time_test) \n",
    "            \n",
    "                assert len(seq_feature_item_test) == seq_length\n",
    "                assert len(seq_feature_time_test) == seq_length\n",
    "                \n",
    "                list_seq_feature_item.append(seq_feature_item_test)\n",
    "                list_seq_feature_time.append(seq_feature_time_test)\n",
    "            \n",
    "            \n",
    "            if len(list_item_id) >= loc + 2:\n",
    "                list_label.append(list_item_id[loc])\n",
    "                list_user.append(user_id)\n",
    "\n",
    "                seq_feature_item = list_item_id[loc+1:loc+seq_length+1] \n",
    "                len_seq_feature_item = len(seq_feature_item)\n",
    "                if len_seq_feature_item < seq_length:\n",
    "                    seq_feature_item += [0] * (seq_length - len_seq_feature_item) \n",
    "                \n",
    "                seq_feature_time = list_time[loc+1:loc+seq_length+1] \n",
    "                len_seq_feature_time = len(seq_feature_time)\n",
    "                if len_seq_feature_time < seq_length:\n",
    "                    seq_feature_time += [0] * (seq_length - len_seq_feature_time) \n",
    "                 \n",
    "                assert len(seq_feature_item) == seq_length\n",
    "                assert len(seq_feature_time) == seq_length\n",
    "                \n",
    "                list_seq_feature_item.append(seq_feature_item)\n",
    "                list_seq_feature_time.append(seq_feature_time)\n",
    "    \n",
    "    df_train_test_label = pd.DataFrame()\n",
    "    df_train_test_label['seq_feature_item'] = list_seq_feature_item\n",
    "    df_train_test_label['seq_feature_time'] = list_seq_feature_time\n",
    "    df_train_test_label['label'] = list_label\n",
    "    df_train_test_label['user_id'] = list_user\n",
    "    \n",
    "        \n",
    "    return df_train_test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18505it [00:03, 5640.15it/s]\n"
     ]
    }
   ],
   "source": [
    "train_test_data = get_data(click_all_gb=click_all_gb,list_label_loc=[0],seq_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id_map</th>\n",
       "      <th>time</th>\n",
       "      <th>flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10007</td>\n",
       "      <td>[9329, 25743, 24599, 20695]</td>\n",
       "      <td>[0.4001817192724083, 0.4461613250459523, 0.608...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id                  item_id_map  \\\n",
       "8   10007  [9329, 25743, 24599, 20695]   \n",
       "\n",
       "                                                time   flag  \n",
       "8  [0.4001817192724083, 0.4461613250459523, 0.608...  train  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "click_all_gb[click_all_gb['user_id']=='10007']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# click_all_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq_feature_item</th>\n",
       "      <th>seq_feature_time</th>\n",
       "      <th>label</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[24599, 25743, 9329, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0.6084817759881395, 0.4461613250459523, 0.400...</td>\n",
       "      <td>20695</td>\n",
       "      <td>10007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            seq_feature_item  \\\n",
       "8  [24599, 25743, 9329, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                    seq_feature_time  label user_id  \n",
       "8  [0.6084817759881395, 0.4461613250459523, 0.400...  20695   10007  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_data[train_test_data['user_id']=='10007']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查看数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq_feature_item</th>\n",
       "      <th>seq_feature_time</th>\n",
       "      <th>label</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[38145, 13095, 20793, 31285, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0.9681528293808541, 0.9474113684510539, 0.947...</td>\n",
       "      <td>-1</td>\n",
       "      <td>1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[7285, 21109, 23274, 13316, 37409, 28767, 0, 0...</td>\n",
       "      <td>[0.8957602694540879, 0.8607619477526436, 0.435...</td>\n",
       "      <td>-1</td>\n",
       "      <td>10032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[16201, 24312, 25696, 10175, 5972, 14039, 2776...</td>\n",
       "      <td>[0.6106346285175493, 0.6102989687147685, 0.610...</td>\n",
       "      <td>-1</td>\n",
       "      <td>10043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[34262, 15116, 26607, 35304, 34453, 39907, 383...</td>\n",
       "      <td>[0.8909684364042963, 0.8909192448814004, 0.890...</td>\n",
       "      <td>-1</td>\n",
       "      <td>10054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>[23876, 34830, 28834, 16109, 22998, 19554, 400...</td>\n",
       "      <td>[0.6541893815762587, 0.6430547356964651, 0.642...</td>\n",
       "      <td>-1</td>\n",
       "      <td>10065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     seq_feature_item  \\\n",
       "10     [38145, 13095, 20793, 31285, 0, 0, 0, 0, 0, 0]   \n",
       "26  [7285, 21109, 23274, 13316, 37409, 28767, 0, 0...   \n",
       "37  [16201, 24312, 25696, 10175, 5972, 14039, 2776...   \n",
       "43  [34262, 15116, 26607, 35304, 34453, 39907, 383...   \n",
       "51  [23876, 34830, 28834, 16109, 22998, 19554, 400...   \n",
       "\n",
       "                                     seq_feature_time  label user_id  \n",
       "10  [0.9681528293808541, 0.9474113684510539, 0.947...     -1    1001  \n",
       "26  [0.8957602694540879, 0.8607619477526436, 0.435...     -1   10032  \n",
       "37  [0.6106346285175493, 0.6102989687147685, 0.610...     -1   10043  \n",
       "43  [0.8909684364042963, 0.8909192448814004, 0.890...     -1   10054  \n",
       "51  [0.6541893815762587, 0.6430547356964651, 0.642...     -1   10065  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_data[train_test_data['label']==-1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id_map</th>\n",
       "      <th>time</th>\n",
       "      <th>flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1001</td>\n",
       "      <td>[31285, 20793, 13095, 38145]</td>\n",
       "      <td>[0.4442544301310222, 0.9471943470260036, 0.947...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   item_id_map  \\\n",
       "10    1001  [31285, 20793, 13095, 38145]   \n",
       "\n",
       "                                                 time  flag  \n",
       "10  [0.4442544301310222, 0.9471943470260036, 0.947...  test  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "click_all_gb[click_all_gb['user_id']=='1001']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id_map</th>\n",
       "      <th>time</th>\n",
       "      <th>flag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10032</td>\n",
       "      <td>[28767, 37409, 13316, 23274, 21109, 7285]</td>\n",
       "      <td>[0.43361459309918204, 0.43391552947352713, 0.4...</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                item_id_map  \\\n",
       "25   10032  [28767, 37409, 13316, 23274, 21109, 7285]   \n",
       "\n",
       "                                                 time  flag  \n",
       "25  [0.43361459309918204, 0.43391552947352713, 0.4...  test  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "click_all_gb[click_all_gb['user_id']=='10032']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq_feature_item</th>\n",
       "      <th>seq_feature_time</th>\n",
       "      <th>label</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[7285, 21109, 23274, 13316, 37409, 28767, 0, 0...</td>\n",
       "      <td>[0.8957602694540879, 0.8607619477526436, 0.435...</td>\n",
       "      <td>-1</td>\n",
       "      <td>10032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[21109, 23274, 13316, 37409, 28767, 0, 0, 0, 0...</td>\n",
       "      <td>[0.8607619477526436, 0.4353825943034243, 0.433...</td>\n",
       "      <td>7285</td>\n",
       "      <td>10032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     seq_feature_item  \\\n",
       "26  [7285, 21109, 23274, 13316, 37409, 28767, 0, 0...   \n",
       "27  [21109, 23274, 13316, 37409, 28767, 0, 0, 0, 0...   \n",
       "\n",
       "                                     seq_feature_time  label user_id  \n",
       "26  [0.8957602694540879, 0.8607619477526436, 0.435...     -1   10032  \n",
       "27  [0.8607619477526436, 0.4353825943034243, 0.433...   7285   10032  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_data[train_test_data['user_id']=='10032']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全分类 双塔 Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnClassificationModel:\n",
    "    def __init__(self, \n",
    "                 seq_length,\n",
    "                 vocab_size,\n",
    "                 embedding_dim,\n",
    "                 output_class_num\n",
    "                ):\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_class_num = output_class_num\n",
    "        \n",
    "        \n",
    "    def model(self):\n",
    "        # 输入\n",
    "        self.inputs_item_seq = tf.keras.Input(shape=(self.seq_length,))\n",
    "        self.inputs_time_seq = tf.keras.Input(shape=(self.seq_length,))\n",
    "        \n",
    "        # embedding 化\n",
    "        self.cat_embedding = tf.keras.layers.Embedding(input_dim=self.vocab_size, \n",
    "                                                       output_dim=self.embedding_dim, \n",
    "                                                       input_length=self.seq_length)(self.inputs_item_seq)\n",
    "        \n",
    "        list_float_embedding = []\n",
    "        split_ = tf.split(self.inputs_time_seq,num_or_size_splits=self.seq_length,axis=-1)\n",
    "        dense_ = tf.keras.layers.Dense(units=self.embedding_dim, activation=tf.nn.relu)\n",
    "        for s in split_:\n",
    "            float_embedding_ = tf.expand_dims(dense_(s),1)\n",
    "            list_float_embedding.append(float_embedding_)\n",
    "        self.float_embedding = tf.concat(list_float_embedding,axis=1)\n",
    "\n",
    "        \n",
    "        # 简单 concat\n",
    "        self.concat = tf.keras.layers.concatenate([self.cat_embedding,self.float_embedding], axis=-1)\n",
    "        \n",
    "        # lstm 网络\n",
    "        self.lstm = tf.keras.layers.LSTM(units=64)(self.concat)\n",
    "        \n",
    "        # fc 网路\n",
    "        self.dense = tf.keras.layers.Dense(units=64, activation=tf.nn.relu)(self.lstm)\n",
    "        \n",
    "        # softmax\n",
    "        self.outputs = tf.keras.layers.Dense(units=self.output_class_num, activation=tf.nn.softmax)(self.dense)\n",
    "        \n",
    "#         print(self.inputs_item_seq,self.inputs_time_seq,self.outputs)\n",
    "        \n",
    "        self.model = tf.keras.Model(inputs=[self.inputs_item_seq,self.inputs_time_seq], outputs=self.outputs)\n",
    "    \n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "vocab_size = max_item_id\n",
    "seq_length = 10\n",
    "output_class_num = max_item_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全分类模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_classification_model = RnnClassificationModel(seq_length=seq_length,\n",
    "                 vocab_size=vocab_size,\n",
    "                 embedding_dim=embedding_dim,\n",
    "                 output_class_num=output_class_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = rnn_classification_model.model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型结构查看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split (TensorFlowOp [(None, 1), (None, 1 0           input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           32          tf_op_layer_split[0][0]          \n",
      "                                                                 tf_op_layer_split[0][1]          \n",
      "                                                                 tf_op_layer_split[0][2]          \n",
      "                                                                 tf_op_layer_split[0][3]          \n",
      "                                                                 tf_op_layer_split[0][4]          \n",
      "                                                                 tf_op_layer_split[0][5]          \n",
      "                                                                 tf_op_layer_split[0][6]          \n",
      "                                                                 tf_op_layer_split[0][7]          \n",
      "                                                                 tf_op_layer_split[0][8]          \n",
      "                                                                 tf_op_layer_split[0][9]          \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims (TensorF [(None, 1, 16)]      0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_1 (Tenso [(None, 1, 16)]      0           dense[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_2 (Tenso [(None, 1, 16)]      0           dense[2][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_3 (Tenso [(None, 1, 16)]      0           dense[3][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_4 (Tenso [(None, 1, 16)]      0           dense[4][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_5 (Tenso [(None, 1, 16)]      0           dense[5][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_6 (Tenso [(None, 1, 16)]      0           dense[6][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_7 (Tenso [(None, 1, 16)]      0           dense[7][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_8 (Tenso [(None, 1, 16)]      0           dense[8][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_9 (Tenso [(None, 1, 16)]      0           dense[9][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 10, 16)       652416      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat (TensorFlowO [(None, 10, 16)]     0           tf_op_layer_ExpandDims[0][0]     \n",
      "                                                                 tf_op_layer_ExpandDims_1[0][0]   \n",
      "                                                                 tf_op_layer_ExpandDims_2[0][0]   \n",
      "                                                                 tf_op_layer_ExpandDims_3[0][0]   \n",
      "                                                                 tf_op_layer_ExpandDims_4[0][0]   \n",
      "                                                                 tf_op_layer_ExpandDims_5[0][0]   \n",
      "                                                                 tf_op_layer_ExpandDims_6[0][0]   \n",
      "                                                                 tf_op_layer_ExpandDims_7[0][0]   \n",
      "                                                                 tf_op_layer_ExpandDims_8[0][0]   \n",
      "                                                                 tf_op_layer_ExpandDims_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 10, 32)       0           embedding[0][0]                  \n",
      "                                                                 tf_op_layer_concat[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 64)           24832       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 64)           4160        lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 40776)        2650440     dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 3,331,880\n",
      "Trainable params: 3,331,880\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型loss 评测 优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_50(y_true, y_pred):\n",
    "    return tf.keras.metrics.sparse_top_k_categorical_accuracy(y_true, y_pred, k=50)\n",
    "def logloss(y_true, y_pred):\n",
    "    return tf.keras.metrics.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "def accuracy(y_true, y_pred):\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=tf.keras.optimizers.RMSprop(),metrics=[logloss,accuracy,top_50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型输入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq_mat1 = np.array(list(train_test_data[train_test_data['label']!=-1]['seq_feature_item']))\n",
    "train_seq_mat2 = np.array(list(train_test_data[train_test_data['label']!=-1]['seq_feature_time']))\n",
    "train_y = np.array(list(train_test_data[train_test_data['label']!=-1]['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14804 samples, validate on 3701 samples\n",
      "Epoch 1/10\n",
      "14804/14804 [==============================] - 33s 2ms/sample - loss: 10.5962 - logloss: 10.5962 - accuracy: 0.0013 - top_50: 0.0143 - val_loss: 10.5774 - val_logloss: 10.5774 - val_accuracy: 8.1059e-04 - val_top_50: 0.0265\n",
      "Epoch 2/10\n",
      "14804/14804 [==============================] - 27s 2ms/sample - loss: 9.9043 - logloss: 9.9043 - accuracy: 0.0016 - top_50: 0.0252 - val_loss: 10.8699 - val_logloss: 10.8699 - val_accuracy: 8.1059e-04 - val_top_50: 0.0284\n",
      "Epoch 3/10\n",
      "14804/14804 [==============================] - 27s 2ms/sample - loss: 9.5838 - logloss: 9.5838 - accuracy: 0.0016 - top_50: 0.0276 - val_loss: 10.9607 - val_logloss: 10.9607 - val_accuracy: 8.1059e-04 - val_top_50: 0.0308\n",
      "Epoch 4/10\n",
      "14804/14804 [==============================] - 25s 2ms/sample - loss: 9.2990 - logloss: 9.2990 - accuracy: 0.0016 - top_50: 0.0290 - val_loss: 11.6556 - val_logloss: 11.6556 - val_accuracy: 8.1059e-04 - val_top_50: 0.0308\n",
      "Epoch 5/10\n",
      "14804/14804 [==============================] - 25s 2ms/sample - loss: 9.0351 - logloss: 9.0351 - accuracy: 0.0020 - top_50: 0.0313 - val_loss: 11.7086 - val_logloss: 11.7086 - val_accuracy: 0.0011 - val_top_50: 0.0303\n",
      "Epoch 6/10\n",
      "14804/14804 [==============================] - 25s 2ms/sample - loss: 8.8374 - logloss: 8.8374 - accuracy: 0.0020 - top_50: 0.0328 - val_loss: 12.1855 - val_logloss: 12.1855 - val_accuracy: 0.0011 - val_top_50: 0.0300\n",
      "Epoch 7/10\n",
      "14804/14804 [==============================] - 23s 2ms/sample - loss: 8.6415 - logloss: 8.6415 - accuracy: 0.0023 - top_50: 0.0367 - val_loss: 12.7017 - val_logloss: 12.7017 - val_accuracy: 0.0016 - val_top_50: 0.0308\n",
      "Epoch 8/10\n",
      "14804/14804 [==============================] - 23s 2ms/sample - loss: 8.4361 - logloss: 8.4361 - accuracy: 0.0020 - top_50: 0.0401 - val_loss: 12.2485 - val_logloss: 12.2485 - val_accuracy: 0.0016 - val_top_50: 0.0267\n",
      "Epoch 9/10\n",
      "14804/14804 [==============================] - 23s 2ms/sample - loss: 8.2154 - logloss: 8.2154 - accuracy: 0.0028 - top_50: 0.0466 - val_loss: 12.7361 - val_logloss: 12.7361 - val_accuracy: 0.0016 - val_top_50: 0.0254\n",
      "Epoch 10/10\n",
      "14804/14804 [==============================] - 23s 2ms/sample - loss: 7.9901 - logloss: 7.9901 - accuracy: 0.0036 - top_50: 0.0589 - val_loss: 12.9526 - val_logloss: 12.9526 - val_accuracy: 0.0011 - val_top_50: 0.0203\n"
     ]
    }
   ],
   "source": [
    "model_fit = model.fit([train_seq_mat1,train_seq_mat2],train_y,batch_size=128,epochs=10,\n",
    "                      validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq_feature_item</th>\n",
       "      <th>seq_feature_time</th>\n",
       "      <th>label</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[32530, 33844, 8333, 8441, 36729, 3209, 18135,...</td>\n",
       "      <td>[0.7062513744688279, 0.6754545875429329, 0.674...</td>\n",
       "      <td>22156</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[935, 2959, 5630, 7634, 35576, 18212, 18813, 1...</td>\n",
       "      <td>[0.9475357940677906, 0.9474142620695385, 0.945...</td>\n",
       "      <td>4678</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[40373, 21953, 25097, 8830, 25706, 26347, 2598...</td>\n",
       "      <td>[0.8525122400087298, 0.8484380244684021, 0.847...</td>\n",
       "      <td>35976</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[35056, 27145, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0.20507945877707037, 0.08657418660304175, 0, ...</td>\n",
       "      <td>35129</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[32430, 29174, 24411, 4714, 22536, 22565, 6081...</td>\n",
       "      <td>[0.9466156232277368, 0.4690440640291869, 0.468...</td>\n",
       "      <td>18311</td>\n",
       "      <td>10002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    seq_feature_item  \\\n",
       "0  [32530, 33844, 8333, 8441, 36729, 3209, 18135,...   \n",
       "1  [935, 2959, 5630, 7634, 35576, 18212, 18813, 1...   \n",
       "2  [40373, 21953, 25097, 8830, 25706, 26347, 2598...   \n",
       "3             [35056, 27145, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4  [32430, 29174, 24411, 4714, 22536, 22565, 6081...   \n",
       "\n",
       "                                    seq_feature_time  label user_id  \n",
       "0  [0.7062513744688279, 0.6754545875429329, 0.674...  22156       1  \n",
       "1  [0.9475357940677906, 0.9474142620695385, 0.945...   4678      10  \n",
       "2  [0.8525122400087298, 0.8484380244684021, 0.847...  35976     100  \n",
       "3  [0.20507945877707037, 0.08657418660304175, 0, ...  35129   10000  \n",
       "4  [0.9466156232277368, 0.4690440640291869, 0.468...  18311   10002  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq_mat1 = np.array(list(train_test_data[train_test_data['label']==-1]['seq_feature_item']))\n",
    "test_seq_mat2 = np.array(list(train_test_data[train_test_data['label']==-1]['seq_feature_time']))\n",
    "# test_y = np.array(list(train_test_data[train_test_data['label']==-1]['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict([test_seq_mat1,test_seq_mat2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 双塔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,num_uids):\n",
    "        super(CustomLayer, self).__init__(trainable=True,dtype=tf.int64)\n",
    "        self.num_uids = num_uids\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.params = tf.Variable(tf.random.normal((num_uids, 9)), trainable=True)\n",
    "        self.built=True\n",
    "\n",
    "    def call(self, input_uid,input_shared_features):\n",
    "        param = tf.gather_nd(self.params, input_uid)\n",
    "        combined = tf.concat([param, input_shared_features], axis=-1)\n",
    "        return combined\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(CustomLayer, self).get_config()\n",
    "        config.update({'num_uids': self.num_uids})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoTowersModel:\n",
    "    def __init__(self, \n",
    "                 seq_length,\n",
    "                 vocab_size,\n",
    "                 embedding_dim\n",
    "                ):\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "    def model(self):\n",
    "        # 输入\n",
    "        # 用户序列特征\n",
    "        self.inputs_item_seq = tf.keras.Input(shape=(self.seq_length,),dtype=tf.int32)\n",
    "        self.inputs_time_seq = tf.keras.Input(shape=(self.seq_length,),dtype=tf.float32)\n",
    "        \n",
    "        # 用户下一个item\n",
    "        self.inputs_item_next = tf.keras.Input(shape=(1,),dtype=tf.int32)\n",
    "        \n",
    "        # seq embedding 化\n",
    "#         param = tf.Variable(tf.zeros([self.vocab_size,self.embedding_dim]))\n",
    "#         self.cat_embedding = tf.nn.embedding_lookup(param, self.inputs_item_seq)\n",
    "        \n",
    "        # item embeeding 化\n",
    "#         self.item_embedding = tf.nn.embedding_lookup(param, self.inputs_item_next)\n",
    "        \n",
    "        self.embedding_ = tf.keras.layers.Embedding(input_dim=self.vocab_size, \n",
    "                                                       output_dim=self.embedding_dim\n",
    "                                                       )\n",
    "        self.cat_embedding = self.embedding_(self.inputs_item_seq)\n",
    "        self.item_embedding = tf.squeeze(self.embedding_(self.inputs_item_next),axis=1)\n",
    "\n",
    "    \n",
    "        # float embedding化\n",
    "        list_float_embedding = []\n",
    "        split_ = tf.split(self.inputs_time_seq,num_or_size_splits=self.seq_length,axis=-1)\n",
    "        dense_ = tf.keras.layers.Dense(units=self.embedding_dim, activation=tf.nn.relu)\n",
    "        for s in split_:\n",
    "            float_embedding_ = tf.expand_dims(dense_(s),1)\n",
    "            list_float_embedding.append(float_embedding_)\n",
    "        self.float_embedding = tf.concat(list_float_embedding,axis=1)\n",
    "\n",
    "#         print(self.cat_embedding,self.float_embedding)\n",
    "        # 简单 concat\n",
    "        self.concat = tf.keras.layers.concatenate([self.cat_embedding,self.float_embedding], axis=-1)\n",
    "        \n",
    "#         print(self.concat)\n",
    "#         self.concat = tf.Tensor(self.concat,dtype=tf.float32)\n",
    "        # lstm 网络\n",
    "        self.lstm = tf.keras.layers.LSTM(units=64)(self.concat)\n",
    "        \n",
    "        # 用户 fc 网路\n",
    "        self.dense_user = tf.keras.layers.Dense(units=64, activation=tf.nn.relu)(self.lstm)\n",
    "        \n",
    "        # 商品 fc 网络\n",
    "        self.dense_item = tf.keras.layers.Dense(units=64, activation=tf.nn.relu)(self.item_embedding)\n",
    "\n",
    "        # a * b\n",
    "        self.similar = tf.reduce_sum(tf.multiply(self.dense_user, self.dense_item),axis=-1)\n",
    "        \n",
    "        # softmax\n",
    "        self.outputs = tf.nn.sigmoid(self.similar)\n",
    "        \n",
    "        # 输出\n",
    "        self.model_user = tf.keras.Model(inputs=[self.inputs_item_seq,self.inputs_time_seq], outputs=self.dense_user)\n",
    "        self.model_item = tf.keras.Model(inputs=[self.inputs_item_next], outputs=self.dense_item)\n",
    "        \n",
    "        self.model = tf.keras.Model(inputs=[self.inputs_item_seq,self.inputs_time_seq,self.inputs_item_next], outputs=self.outputs)\n",
    "    \n",
    "        return self.model, self.model_user, self.model_item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 双塔模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_towers_model = TwoTowersModel(seq_length=seq_length,\n",
    "                 vocab_size=vocab_size,\n",
    "                 embedding_dim=embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2, model2_user, model2_item = two_towers_model.model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查看 商品item 模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      multiple                  652416    \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Squeeze (TensorF [(None, 16)]              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                1088      \n",
      "=================================================================\n",
      "Total params: 653,504\n",
      "Trainable params: 653,504\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2_item.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查看 用户user 模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_1 (TensorFlow [(None, 1), (None, 1 0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           32          tf_op_layer_split_1[0][0]        \n",
      "                                                                 tf_op_layer_split_1[0][1]        \n",
      "                                                                 tf_op_layer_split_1[0][2]        \n",
      "                                                                 tf_op_layer_split_1[0][3]        \n",
      "                                                                 tf_op_layer_split_1[0][4]        \n",
      "                                                                 tf_op_layer_split_1[0][5]        \n",
      "                                                                 tf_op_layer_split_1[0][6]        \n",
      "                                                                 tf_op_layer_split_1[0][7]        \n",
      "                                                                 tf_op_layer_split_1[0][8]        \n",
      "                                                                 tf_op_layer_split_1[0][9]        \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_10 (Tens [(None, 1, 16)]      0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_11 (Tens [(None, 1, 16)]      0           dense_3[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_12 (Tens [(None, 1, 16)]      0           dense_3[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_13 (Tens [(None, 1, 16)]      0           dense_3[3][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_14 (Tens [(None, 1, 16)]      0           dense_3[4][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_15 (Tens [(None, 1, 16)]      0           dense_3[5][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_16 (Tens [(None, 1, 16)]      0           dense_3[6][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_17 (Tens [(None, 1, 16)]      0           dense_3[7][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_18 (Tens [(None, 1, 16)]      0           dense_3[8][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_19 (Tens [(None, 1, 16)]      0           dense_3[9][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         multiple             652416      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_1 (TensorFlo [(None, 10, 16)]     0           tf_op_layer_ExpandDims_10[0][0]  \n",
      "                                                                 tf_op_layer_ExpandDims_11[0][0]  \n",
      "                                                                 tf_op_layer_ExpandDims_12[0][0]  \n",
      "                                                                 tf_op_layer_ExpandDims_13[0][0]  \n",
      "                                                                 tf_op_layer_ExpandDims_14[0][0]  \n",
      "                                                                 tf_op_layer_ExpandDims_15[0][0]  \n",
      "                                                                 tf_op_layer_ExpandDims_16[0][0]  \n",
      "                                                                 tf_op_layer_ExpandDims_17[0][0]  \n",
      "                                                                 tf_op_layer_ExpandDims_18[0][0]  \n",
      "                                                                 tf_op_layer_ExpandDims_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 10, 32)       0           embedding_1[0][0]                \n",
      "                                                                 tf_op_layer_concat_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 64)           24832       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64)           4160        lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 681,440\n",
      "Trainable params: 681,440\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2_user.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查看整体模型结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_1 (TensorFlow [(None, 1), (None, 1 0           input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 16)           32          tf_op_layer_split_1[0][0]        \n",
      "                                                                 tf_op_layer_split_1[0][1]        \n",
      "                                                                 tf_op_layer_split_1[0][2]        \n",
      "                                                                 tf_op_layer_split_1[0][3]        \n",
      "                                                                 tf_op_layer_split_1[0][4]        \n",
      "                                                                 tf_op_layer_split_1[0][5]        \n",
      "                                                                 tf_op_layer_split_1[0][6]        \n",
      "                                                                 tf_op_layer_split_1[0][7]        \n",
      "                                                                 tf_op_layer_split_1[0][8]        \n",
      "                                                                 tf_op_layer_split_1[0][9]        \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_10 (Tens [(None, 1, 16)]      0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_11 (Tens [(None, 1, 16)]      0           dense_3[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_12 (Tens [(None, 1, 16)]      0           dense_3[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_13 (Tens [(None, 1, 16)]      0           dense_3[3][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_14 (Tens [(None, 1, 16)]      0           dense_3[4][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_15 (Tens [(None, 1, 16)]      0           dense_3[5][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_16 (Tens [(None, 1, 16)]      0           dense_3[6][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_17 (Tens [(None, 1, 16)]      0           dense_3[7][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_18 (Tens [(None, 1, 16)]      0           dense_3[8][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_19 (Tens [(None, 1, 16)]      0           dense_3[9][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         multiple             652416      input_3[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_1 (TensorFlo [(None, 10, 16)]     0           tf_op_layer_ExpandDims_10[0][0]  \n",
      "                                                                 tf_op_layer_ExpandDims_11[0][0]  \n",
      "                                                                 tf_op_layer_ExpandDims_12[0][0]  \n",
      "                                                                 tf_op_layer_ExpandDims_13[0][0]  \n",
      "                                                                 tf_op_layer_ExpandDims_14[0][0]  \n",
      "                                                                 tf_op_layer_ExpandDims_15[0][0]  \n",
      "                                                                 tf_op_layer_ExpandDims_16[0][0]  \n",
      "                                                                 tf_op_layer_ExpandDims_17[0][0]  \n",
      "                                                                 tf_op_layer_ExpandDims_18[0][0]  \n",
      "                                                                 tf_op_layer_ExpandDims_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 10, 32)       0           embedding_1[0][0]                \n",
      "                                                                 tf_op_layer_concat_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 64)           24832       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Squeeze (TensorFlow [(None, 16)]         0           embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 64)           4160        lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 64)           1088        tf_op_layer_Squeeze[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Mul (TensorFlowOpLa [(None, 64)]         0           dense_4[0][0]                    \n",
      "                                                                 dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum (TensorFlowOpLa [(None,)]            0           tf_op_layer_Mul[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sigmoid (TensorFlow [(None,)]            0           tf_op_layer_Sum[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 682,528\n",
      "Trainable params: 682,528\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型 loss 评测 优化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss=tf.keras.losses.binary_crossentropy,optimizer=tf.keras.optimizers.RMSprop(),metrics=[tf.keras.metrics.binary_crossentropy,tf.keras.metrics.binary_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(X1,X2,y,batch_size=128):\n",
    "    # seq_feature_item\tseq_feature_time\tlabel\n",
    "    length = len(list(y))\n",
    "    index = 0\n",
    "    while True:\n",
    "        index += 1\n",
    "        if index > 1000:\n",
    "            break\n",
    "        \n",
    "        sample_index1 = np.random.randint(length, size=batch_size)\n",
    "        X1_1 = X1[sample_index1]\n",
    "        X2_1 = X2[sample_index1]\n",
    "        y_1 = y[sample_index1]\n",
    "        \n",
    "        sample_index2 = np.random.randint(length, size=batch_size)\n",
    "        y_0 = y[sample_index2]\n",
    "        \n",
    "        label_1_0 = np.array([1] * batch_size + [1 if i==j else 0 for i,j in zip(y_1,y_0)]) \n",
    "        \n",
    "        yield [np.vstack([X1_1,X1_1]), np.vstack([X2_1,X2_1]), np.hstack([y_1,y_0])], label_1_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试迭代器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=next(generator(train_seq_mat1,train_seq_mat2,train_y,batch_size=128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([[28317,  1206, 35574, ...,  6019,   642, 39609],\n",
       "         [15055, 11327, 38786, ..., 17432, 15830,  4605],\n",
       "         [ 9569, 23903,  6016, ...,  9926, 16662,  4215],\n",
       "         ...,\n",
       "         [33592, 25060, 28384, ...,     0,     0,     0],\n",
       "         [35266, 19392, 14210, ...,     0,     0,     0],\n",
       "         [21604, 30239, 15968, ...,     0,     0,     0]]),\n",
       "  array([[0.62669132, 0.62631804, 0.46359827, ..., 0.46310346, 0.46307453,\n",
       "          0.4630398 ],\n",
       "         [0.9843571 , 0.98363658, 0.98315914, ..., 0.95427793, 0.9542432 ,\n",
       "          0.95396542],\n",
       "         [0.90602394, 0.90579534, 0.85741982, ..., 0.37876026, 0.31506592,\n",
       "          0.31484021],\n",
       "         ...,\n",
       "         [0.46348542, 0.46268968, 0.46133546, ..., 0.        , 0.        ,\n",
       "          0.        ],\n",
       "         [0.65940079, 0.47213734, 0.34166696, ..., 0.        , 0.        ,\n",
       "          0.        ],\n",
       "         [0.6989392 , 0.6973535 , 0.21630381, ..., 0.        , 0.        ,\n",
       "          0.        ]]),\n",
       "  array([27170,  2747, 12380, 31505,  2696, 29174, 25842,  5827, 23878,\n",
       "         18668, 38562,  4989, 40251, 18449, 23100, 12663, 23978, 18022,\n",
       "          2358,  1246, 12007, 11051, 35136, 21003,  8140, 40102,  2384,\n",
       "         17691, 36040,  9952,  6724, 25591, 35881, 23679, 20135,  4143,\n",
       "           811, 38820, 19089,  8585, 14888, 34193, 28052,  7221, 29590,\n",
       "         13357,  1895, 11377, 39229,  3138, 16012, 13465,  9376,  9650,\n",
       "         37580, 28986,  7798, 33134, 38803, 20031, 36683,  5994, 29871,\n",
       "         18515, 34556, 36077, 18267,  8693,  7942, 40690, 11400, 18470,\n",
       "         38700, 25595,  2511, 27836, 38997, 40120, 35849,  5829, 24210,\n",
       "          9555, 17933,  2973,  1641,  7285, 12160, 10438, 26733, 28659,\n",
       "         18490, 15423, 27871, 10290, 15641, 35262, 21992, 18884,  4182,\n",
       "         15729,  7626, 16392, 16976,  7992,   630,  7563,   283, 35475,\n",
       "         38016, 26722, 38786, 37235, 20634,  7973, 10010,  8216, 20093,\n",
       "         26716, 18658,  7931, 33384, 10162, 12194, 31201, 16971, 35586,\n",
       "          9660,  2946, 16336, 38152, 17334, 30107, 22049, 33803, 25521,\n",
       "         23881, 36042, 22380, 28768, 35344, 21251, 19703, 19380, 14708,\n",
       "         40354,  1057, 31213, 37116, 19356, 36042, 16396, 39709, 31991,\n",
       "         18297, 22764,  5177, 27726, 24899, 35231, 33398, 36940, 12195,\n",
       "         32582, 30903, 11372, 32599,  2411, 10346, 31210, 21775, 25842,\n",
       "         24836,  3646, 20651, 39990, 15765, 12300,  7718, 15824, 38683,\n",
       "         19514, 34425, 15926, 33455,  9555, 24042, 24492,  5676, 15737,\n",
       "         30676, 39180, 20413, 18969, 32618,  1705, 14387, 33563, 12838,\n",
       "         37409, 34463, 20800, 37197, 38386,  1457, 35526, 20973, 14915,\n",
       "          3343, 35886, 19241,  8336,  5814,  3293, 12700, 17990, 37874,\n",
       "          7960, 28724, 11084, 34233, 40113, 33278, 15111, 19666,  7455,\n",
       "         29813, 16331,  7823, 38960, 40403, 34364, 18343,  2836, 16464,\n",
       "         10281,  8824, 23157, 19998, 31002, 11685,  2737, 15332, 33403,\n",
       "         26030,  2082, 10162, 14892,  5181, 15927, 23523, 22247, 22506,\n",
       "         14708, 20763, 36067,   552])],\n",
       " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 256 samples\n",
      "256/256 [==============================] - 3s 10ms/sample - loss: 0.6932 - binary_crossentropy: 0.6932 - binary_accuracy: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x142f749e8>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit([data[0][0],data[0][1],data[0][2]],data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-65-9fef89a4f17a>:1: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "1000/1000 [==============================] - 40s 40ms/step - loss: 0.6795 - binary_crossentropy: 0.6795 - binary_accuracy: 0.5398\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x143bf7d68>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit_generator(generator(train_seq_mat1,train_seq_mat2,train_y,batch_size=128),epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用户user 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq_mat1 = np.array(list(train_test_data[train_test_data['label']==-1]['seq_feature_item']))\n",
    "test_seq_mat2 = np.array(list(train_test_data[train_test_data['label']==-1]['seq_feature_time']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embedding = model2_user.predict([test_seq_mat1,test_seq_mat2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查看用户user embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.18476534],\n",
       "       [0.        , 0.        , 0.05551466, ..., 0.02946857, 0.32896775,\n",
       "        0.0164078 ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.2463957 ,\n",
       "        0.05669748],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.15368362,\n",
       "        0.07387834],\n",
       "       [0.02807279, 0.        , 0.        , ..., 0.04799657, 0.33363134,\n",
       "        0.02776098],\n",
       "       [0.        , 0.56821394, 0.8854616 , ..., 0.        , 0.        ,\n",
       "        0.38548687]], dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 商品item 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_mat = np.array(list(train_test_data[train_test_data['label']!=-1]['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_embedding = model2_item.predict([item_mat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 查看商品item embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.0324979 ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.09356035,\n",
       "        0.        ],\n",
       "       [0.        , 0.05674259, 0.        , ..., 0.        , 0.03220441,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.07289328,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.03572592,\n",
       "        0.        ],\n",
       "       [0.01450562, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rrank 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankModel:\n",
    "    def __init__(self, \n",
    "                 seq_length,\n",
    "                 vocab_size,\n",
    "                 embedding_dim\n",
    "                ):\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "    def model(self):\n",
    "        # 输入\n",
    "        # 用户序列特征\n",
    "        self.inputs_item_seq = tf.keras.Input(shape=(self.seq_length,),dtype=tf.int32)\n",
    "        self.inputs_time_seq = tf.keras.Input(shape=(self.seq_length,),dtype=tf.float32)\n",
    "        \n",
    "        # 用户下一个item\n",
    "        self.inputs_item_next = tf.keras.Input(shape=(1,),dtype=tf.int32)\n",
    "        \n",
    "        # seq embedding 化\n",
    "#         param = tf.Variable(tf.zeros([self.vocab_size,self.embedding_dim]))\n",
    "#         self.cat_embedding = tf.nn.embedding_lookup(param, self.inputs_item_seq)\n",
    "        \n",
    "        # item embeeding 化\n",
    "#         self.item_embedding = tf.nn.embedding_lookup(param, self.inputs_item_next)\n",
    "        \n",
    "        self.embedding_ = tf.keras.layers.Embedding(input_dim=self.vocab_size, \n",
    "                                                       output_dim=self.embedding_dim\n",
    "                                                       )\n",
    "        self.cat_embedding = self.embedding_(self.inputs_item_seq)\n",
    "        self.item_embedding = tf.squeeze(self.embedding_(self.inputs_item_next),axis=1)\n",
    "\n",
    "    \n",
    "        # float embedding化\n",
    "        list_float_embedding = []\n",
    "        split_ = tf.split(self.inputs_time_seq,num_or_size_splits=self.seq_length,axis=-1)\n",
    "        dense_ = tf.keras.layers.Dense(units=self.embedding_dim, activation=tf.nn.relu)\n",
    "        for s in split_:\n",
    "            float_embedding_ = tf.expand_dims(dense_(s),1)\n",
    "            list_float_embedding.append(float_embedding_)\n",
    "        self.float_embedding = tf.concat(list_float_embedding,axis=1)\n",
    "\n",
    "#         print(self.cat_embedding,self.float_embedding)\n",
    "        # 简单 concat\n",
    "        self.concat = tf.keras.layers.concatenate([self.cat_embedding,self.float_embedding],axis=-1)\n",
    "        \n",
    "#         print(self.concat)\n",
    "#         self.concat = tf.Tensor(self.concat,dtype=tf.float32)\n",
    "        # lstm 网络\n",
    "        self.lstm = tf.keras.layers.LSTM(units=64)(self.concat)\n",
    "        \n",
    "        # 用户 fc 网路\n",
    "        self.dense_user = tf.keras.layers.Dense(units=64, activation=tf.nn.relu)(self.lstm)\n",
    "        \n",
    "        # 商品 fc 网络\n",
    "        self.dense_item = tf.keras.layers.Dense(units=64, activation=tf.nn.relu)(self.item_embedding)\n",
    "        \n",
    "        # 合并网络\n",
    "        self.dense = tf.keras.layers.concatenate([self.dense_user,self.dense_item],axis=-1)\n",
    "            \n",
    "        # softmax\n",
    "        self.outputs = tf.keras.layers.Dense(units=1, activation=tf.nn.sigmoid)(self.dense)\n",
    "        \n",
    "        \n",
    "        # 输出\n",
    "        # self.model_user = tf.keras.Model(inputs=[self.inputs_item_seq,self.inputs_time_seq], outputs=self.dense_user)\n",
    "        # self.model_item = tf.keras.Model(inputs=[self.inputs_item_next], outputs=self.dense_item)\n",
    "        \n",
    "        self.model = tf.keras.Model(inputs=[self.inputs_item_seq,self.inputs_time_seq,self.inputs_item_next], outputs=self.outputs)\n",
    "    \n",
    "        return self.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 排序模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_model = RankModel(seq_length=seq_length,\n",
    "                 vocab_size=vocab_size,\n",
    "                 embedding_dim=embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = rank_model.model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型结构查看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split_2 (TensorFlow [(None, 1), (None, 1 0           input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 16)           32          tf_op_layer_split_2[0][0]        \n",
      "                                                                 tf_op_layer_split_2[0][1]        \n",
      "                                                                 tf_op_layer_split_2[0][2]        \n",
      "                                                                 tf_op_layer_split_2[0][3]        \n",
      "                                                                 tf_op_layer_split_2[0][4]        \n",
      "                                                                 tf_op_layer_split_2[0][5]        \n",
      "                                                                 tf_op_layer_split_2[0][6]        \n",
      "                                                                 tf_op_layer_split_2[0][7]        \n",
      "                                                                 tf_op_layer_split_2[0][8]        \n",
      "                                                                 tf_op_layer_split_2[0][9]        \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_20 (Tens [(None, 1, 16)]      0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_21 (Tens [(None, 1, 16)]      0           dense_6[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_22 (Tens [(None, 1, 16)]      0           dense_6[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_23 (Tens [(None, 1, 16)]      0           dense_6[3][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_24 (Tens [(None, 1, 16)]      0           dense_6[4][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_25 (Tens [(None, 1, 16)]      0           dense_6[5][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_26 (Tens [(None, 1, 16)]      0           dense_6[6][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_27 (Tens [(None, 1, 16)]      0           dense_6[7][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_28 (Tens [(None, 1, 16)]      0           dense_6[8][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_29 (Tens [(None, 1, 16)]      0           dense_6[9][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         multiple             652416      input_6[0][0]                    \n",
      "                                                                 input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_2 (TensorFlo [(None, 10, 16)]     0           tf_op_layer_ExpandDims_20[0][0]  \n",
      "                                                                 tf_op_layer_ExpandDims_21[0][0]  \n",
      "                                                                 tf_op_layer_ExpandDims_22[0][0]  \n",
      "                                                                 tf_op_layer_ExpandDims_23[0][0]  \n",
      "                                                                 tf_op_layer_ExpandDims_24[0][0]  \n",
      "                                                                 tf_op_layer_ExpandDims_25[0][0]  \n",
      "                                                                 tf_op_layer_ExpandDims_26[0][0]  \n",
      "                                                                 tf_op_layer_ExpandDims_27[0][0]  \n",
      "                                                                 tf_op_layer_ExpandDims_28[0][0]  \n",
      "                                                                 tf_op_layer_ExpandDims_29[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "input_8 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 10, 32)       0           embedding_2[0][0]                \n",
      "                                                                 tf_op_layer_concat_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 64)           24832       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Squeeze_1 (TensorFl [(None, 16)]         0           embedding_2[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 64)           4160        lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 64)           1088        tf_op_layer_Squeeze_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 128)          0           dense_7[0][0]                    \n",
      "                                                                 dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 1)            129         concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 682,657\n",
      "Trainable params: 682,657\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 赠送大家 封装的基础结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征配置文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigFeatures:\n",
    "    def __init__(self,\n",
    "                features_name,\n",
    "                group_name,\n",
    "                features_type,\n",
    "                features_length,\n",
    "                data_type,\n",
    "                vocab_size):\n",
    "        self.features_name = features_name\n",
    "        self.group_name = group_name\n",
    "        self.features_type = features_type\n",
    "        self.features_length = int(features_length)\n",
    "        self.data_type = data_type\n",
    "        self.vocab_size = int(vocab_size) + 1\n",
    "        self.deal_type()\n",
    "    \n",
    "    def deal_type(self):\n",
    "        type_map = {\n",
    "                    'int8':tf.int8,\n",
    "                    'int16':tf.int16,\n",
    "                    'int32':tf.int32,\n",
    "                    'int64':tf.int64,\n",
    "                    'float16':tf.float16,\n",
    "                    'float32':tf.float32,\n",
    "                    'float64':tf.float64,\n",
    "                    'double':tf.double\n",
    "                    }\n",
    "        self.data_type = type_map[self.data_type]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据输入文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Input:\n",
    "    def __init__(self,\n",
    "                 config_features\n",
    "                ):\n",
    "        self.config_features = config_features\n",
    "        \n",
    "    def __call__(self):\n",
    "        tf_input = tf.keras.Input(shape=self.config_features.features_length, \n",
    "                       name='input_'+self.config_features.features_name, \n",
    "                       dtype=self.config_features.data_type)\n",
    "        return tf_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseEmbeeding:\n",
    "    def __init__(self,\n",
    "                output_dim,\n",
    "                activation=None, \n",
    "                use_bias=True,\n",
    "                trainable=True\n",
    "                ):\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "        self.trainable = trainable\n",
    "    def __call__(self, input_layer):\n",
    "        output_layer = tf.keras.layers.Dense(\n",
    "                            units=self.output_dim,\n",
    "                            activation=self.activation,\n",
    "                            use_bias=self.use_bias,\n",
    "                            trainable=self.trainable,\n",
    "                            )(input_layer)\n",
    "        return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoryEmbeeding:\n",
    "    def __init__(self,\n",
    "                output_dim,\n",
    "                vocab_size,\n",
    "                embeddings_initializer='uniform',\n",
    "                trainable=True\n",
    "                ):\n",
    "        self.output_dim = output_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embeddings_initializer = embeddings_initializer\n",
    "        self.trainable = trainable\n",
    "    def __call__(self, input_layer):\n",
    "        output_layer = tf.keras.layers.Embedding(\n",
    "                        input_dim=self.vocab_size, \n",
    "                        output_dim=self.output_dim,\n",
    "                        embeddings_initializer=self.embeddings_initializer,\n",
    "                        trainable=self.trainable,\n",
    "                        )(input_layer)\n",
    "        output_layer = tf.squeeze(output_layer,axis=1)\n",
    "        return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseSequenceEmbeeding:\n",
    "    def __init__(self,\n",
    "                output_dim,\n",
    "                activation=None, \n",
    "                use_bias=True,\n",
    "                trainable=True\n",
    "                ):\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "        self.trainable = trainable\n",
    "    def __call__(self, input_layer):\n",
    "        input_layer = tf.expand_dims(input_layer,axis=-1)\n",
    "        output_layer = tf.keras.layers.Dense(\n",
    "                            units=self.output_dim,\n",
    "                            activation=self.activation,\n",
    "                            use_bias=self.use_bias,\n",
    "                            trainable=self.trainable,\n",
    "                            )(input_layer)\n",
    "        return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategorySequenceEmbeeding:\n",
    "    def __init__(self,\n",
    "                output_dim,\n",
    "                vocab_size,\n",
    "                embeddings_initializer='uniform',\n",
    "                trainable=True\n",
    "                ):\n",
    "        self.output_dim = output_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embeddings_initializer = embeddings_initializer\n",
    "        self.trainable = trainable\n",
    "    def __call__(self, input_layer):\n",
    "        output_layer = tf.keras.layers.Embedding(\n",
    "                        input_dim=self.vocab_size, \n",
    "                        output_dim=self.output_dim,\n",
    "                        embeddings_initializer=self.embeddings_initializer,\n",
    "                        trainable=self.trainable,\n",
    "                        )(input_layer)\n",
    "        return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategorySequenceEmbeeding_same:\n",
    "    def __init__(self,\n",
    "                output_dim,\n",
    "                vocab_size,\n",
    "                embeddings_initializer='uniform',\n",
    "                trainable=True\n",
    "                ):\n",
    "        self.output_dim = output_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embeddings_initializer = embeddings_initializer\n",
    "        self.trainable = trainable\n",
    "    def __call__(self, input_layer):\n",
    "        output_layer = tf.keras.layers.Embedding(\n",
    "                        input_dim=self.vocab_size, \n",
    "                        output_dim=self.output_dim,\n",
    "                        embeddings_initializer=self.embeddings_initializer,\n",
    "                        trainable=self.trainable,\n",
    "                        )(input_layer)\n",
    "        return output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FmLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FmLayer:\n",
    "    def __init__(self,\n",
    "                dense_use=True, \n",
    "                dense_units=8, \n",
    "                activation=tf.nn.relu,\n",
    "                use_bias=True,\n",
    "                trainable=True,\n",
    "                reduce_sum=True\n",
    "                ):\n",
    "        self.dense_use = dense_use\n",
    "        self.dense_units = dense_units\n",
    "        self.activation = activation\n",
    "        self.reduce_sum = reduce_sum\n",
    "        self.use_bias = use_bias\n",
    "        self.trainable = trainable\n",
    "        \n",
    "    def __call__(self, list_input_layer):\n",
    "\n",
    "        list_interaction = []\n",
    "        for input_layer_i in list_input_layer:\n",
    "            for input_layer_j in list_input_layer:\n",
    "                \n",
    "                assert len(input_layer_i.get_shape().as_list()) == 2\n",
    "                assert len(input_layer_j.get_shape().as_list()) == 2\n",
    "                \n",
    "                interaction_layer = input_layer_i * input_layer_j\n",
    "                if self.dense_use:\n",
    "                    interaction_layer = tf.keras.layers.Dense(\n",
    "                            units=self.dense_units,\n",
    "                            activation=self.activation,\n",
    "                            use_bias=self.use_bias,\n",
    "                            trainable=self.trainable,\n",
    "                            )(interaction_layer)\n",
    "                if self.reduce_sum:\n",
    "                    interaction_layer = tf.reduce_sum(interaction_layer,axis=-1)\n",
    "                if len(interaction_layer.get_shape().as_list()) == 1:\n",
    "                    interaction_layer = tf.expand_dims(interaction_layer,axis=-1)\n",
    "                    \n",
    "                list_interaction.append(interaction_layer)\n",
    "        return tf.concat(list_interaction,axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self,\n",
    "                list_dense_units,\n",
    "                activation=tf.nn.relu,\n",
    "                use_bias=True,\n",
    "                trainable=True\n",
    "                ):\n",
    "        self.list_dense_units = list_dense_units\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "        self.trainable = trainable\n",
    "    \n",
    "    def __call__(self, list_input_layer):\n",
    "        output_layer = tf.concat(list_input_layer,axis=-1)\n",
    "        \n",
    "        assert len(output_layer.get_shape().as_list()) == 2\n",
    "        for units in self.list_dense_units:\n",
    "            output_layer = tf.keras.layers.Dense(\n",
    "                            units=units,\n",
    "                            activation=self.activation,\n",
    "                            use_bias=self.use_bias,\n",
    "                            trainable=self.trainable,\n",
    "                            )(output_layer)\n",
    "        return output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AddMultiDenseLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddMultiDenseLayer:\n",
    "    def __init__(self,\n",
    "                activation=tf.nn.relu,\n",
    "                use_bias=True,\n",
    "                trainable=True\n",
    "                ):\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "        self.trainable = trainable \n",
    "        \n",
    "    def __call__(self, input_layer):\n",
    "        input_layer = input_layer\n",
    "        assert len(input_layer.get_shape().as_list()) == 2\n",
    "        \n",
    "        layer_add = input_layer + tf.keras.layers.Dense(\n",
    "                            units=input_layer.get_shape().as_list()[-1],\n",
    "                            activation=self.activation,\n",
    "                            use_bias=self.use_bias,\n",
    "                            trainable=self.trainable,\n",
    "                            )(input_layer)\n",
    "        \n",
    "        layer_multi = input_layer * tf.keras.layers.Dense(\n",
    "                            units=input_layer.get_shape().as_list()[-1],\n",
    "                            activation=self.activation,\n",
    "                            use_bias=self.use_bias,\n",
    "                            trainable=self.trainable,\n",
    "                            )(input_layer)\n",
    "        output_layer = tf.concat([layer_add,layer_multi])\n",
    "        \n",
    "        output_layer = tf.keras.layers.Dense(\n",
    "                            units=input_layer.get_shape().as_list()[-1],\n",
    "                            activation=self.activation,\n",
    "                            use_bias=self.use_bias,\n",
    "                            trainable=self.trainable,\n",
    "                            )(output_layer)\n",
    "        return output_layer, input_layer + output_layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DinLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DinLayer:\n",
    "    def __init__(self,\n",
    "                activation=tf.nn.relu,\n",
    "                use_bias=True,\n",
    "                trainable=True,\n",
    "                use_dense=True\n",
    "                ):\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "        self.trainable = trainable\n",
    "        self.use_dense = use_dense\n",
    "    def __call__(self, \n",
    "                 list_input_sequence_layer,\n",
    "                 list_input_layer\n",
    "                ):\n",
    "        output_sequence_layer = tf.concat(list_input_sequence_layer,axis=-1)\n",
    "        _, sequence_length, units = output_sequence_layer.get_shape().as_list()\n",
    "        \n",
    "        \n",
    "        output_layer = tf.concat(list_input_layer,axis=-1)\n",
    "        output_layer = tf.keras.layers.Dense(\n",
    "                            units=units,\n",
    "                            activation=self.activation,\n",
    "                            use_bias=self.use_bias,\n",
    "                            trainable=self.trainable,\n",
    "                            )(output_layer)\n",
    "        \n",
    "        \n",
    "        list_layer = []\n",
    "        for layer_ in tf.split(output_sequence_layer,sequence_length,axis=1):\n",
    "            \n",
    "            layer_ = tf.squeeze(layer_,axis=1)\n",
    "            layer = layer_\n",
    "            if self.use_dense:\n",
    "                layer_ = tf.keras.layers.Dense(\n",
    "                            units=units,\n",
    "                            activation=self.activation,\n",
    "                            use_bias=self.use_bias,\n",
    "                            trainable=self.trainable,\n",
    "                            )(layer_)\n",
    "                \n",
    "            dot_ = output_layer * layer_\n",
    "            concat_ = tf.concat([layer_, output_layer, dot_],axis=-1)\n",
    "            \n",
    "            active_units_ = tf.keras.layers.Dense(\n",
    "                            units=1,\n",
    "                            activation=self.activation,\n",
    "                            use_bias=self.use_bias,\n",
    "                            trainable=self.trainable,\n",
    "                            )(concat_)\n",
    "            layer = tf.expand_dims(layer * active_units_,-1)\n",
    "            list_layer.append(layer)\n",
    "            \n",
    "        output_layer = tf.reduce_sum(tf.concat(list_layer,axis=-1),axis=-1)\n",
    "        \n",
    "        return output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DinSameLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DinSameLayer:\n",
    "    def __init__(self,\n",
    "                activation=tf.nn.relu,\n",
    "                use_bias=True,\n",
    "                trainable=True,\n",
    "                use_dense=True\n",
    "                ):\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "        self.trainable = trainable\n",
    "        self.use_dense = use_dense\n",
    "    def __call__(self, \n",
    "                 list_input_sequence_layer,\n",
    "                 list_input_layer\n",
    "                ):\n",
    "        output_sequence_layer = tf.concat(list_input_sequence_layer,axis=-1)\n",
    "        _, sequence_length, units = output_sequence_layer.get_shape().as_list()\n",
    "        \n",
    "        \n",
    "        output_layer = tf.concat(list_input_layer,axis=-1)\n",
    "        output_layer = tf.keras.layers.Dense(\n",
    "                            units=units,\n",
    "                            activation=self.activation,\n",
    "                            use_bias=self.use_bias,\n",
    "                            trainable=self.trainable,\n",
    "                            )(output_layer)\n",
    "        \n",
    "        \n",
    "        list_layer = []\n",
    "        dense_layer_1 = tf.keras.layers.Dense(\n",
    "                            units=units,\n",
    "                            activation=self.activation,\n",
    "                            use_bias=self.use_bias,\n",
    "                            trainable=self.trainable,\n",
    "                            )\n",
    "        dense_layer_2 = tf.keras.layers.Dense(\n",
    "                            units=1,\n",
    "                            activation=self.activation,\n",
    "                            use_bias=self.use_bias,\n",
    "                            trainable=self.trainable,\n",
    "                            )\n",
    "        for layer_ in tf.split(output_sequence_layer,sequence_length,axis=1):\n",
    "            \n",
    "            layer_ = tf.squeeze(layer_,axis=1)\n",
    "            layer = layer_\n",
    "            if self.use_dense:\n",
    "                layer_ = dense_layer_1(layer_)\n",
    "                \n",
    "            dot_ = output_layer * layer_\n",
    "            concat_ = tf.concat([layer_, output_layer, dot_],axis=-1)\n",
    "            \n",
    "            active_units_ = dense_layer_2(concat_)\n",
    "            layer = tf.expand_dims(layer * active_units_,-1)\n",
    "            list_layer.append(layer)\n",
    "            \n",
    "        output_layer = tf.reduce_sum(tf.concat(list_layer,axis=-1),axis=-1)\n",
    "        \n",
    "        return output_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
